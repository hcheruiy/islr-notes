{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "Though linear regression can be applied in the case of binary qualitative responses, difficulties arise beyond two levels. For example, choosing a coding scheme is problematic and different coding scheme can yield wildly different predictions.\n",
    "\n",
    "## Logistic Regression\n",
    "Logistic regression models the probability that $y$ belongs to a particular category rather than modeling the response itself.\n",
    "\n",
    "Logistic regression uses the logistic function to ensure a prediction between 0 and 1. The logistic function takes the form\n",
    "\n",
    "$$p(\\mathbf{X}) = \\frac{e^{\\beta_0 + \\beta_1\\mathbf{X}}}{1 + e^{\\beta_0 + \\beta_1\\mathbf{X}}}.$$\n",
    "This yields a probability greater than 0 and less than 1.\n",
    "\n",
    "The logistic function can be rebalanced to yield\n",
    "\n",
    "$$\n",
    "\\frac{p(\\mathbf{X})}{1−p(\\mathbf{X})} = e^{\\beta_0 + \\beta_1\\mathbf{X}}\n",
    "$$\n",
    "$\\frac{p(\\mathbf{X})}{1−p(\\mathbf{X})}$ is known as the __odds__ and takes on a value between 0 and infinity.\n",
    "\n",
    "As an example, a probability of 1 in 5 yields odds of $\\frac{1}{4}$ since $\\frac{0.2}{1−0.2} = \\frac{1}{4}.$\n",
    "\n",
    "Taking a logarithm of both sides of the logistic odds equation yields an equation for the __log-odds__ or __logit__,\n",
    "\n",
    "$$\\text{log}\\bigg(\\frac{p(\\mathbf{X})}{1−p(\\mathbf{X})}\\bigg) = \\beta_0 + \\beta_1\\mathbf{X}$$\n",
    "Logistic regression has a logit that is linear in terms of $\\mathbf{X}$.\n",
    "\n",
    "Unlike linear regression where $\\beta_1$ represents the average change in $\\mathbf{Y}$ with a one-unit increase in $\\mathbf{X}$, for logistic regression, increasing $\\mathbf{X}$ by one-unit yields a $\\beta_1$ change in the log-odds which is equivalent to multiplying the odds by $e\\beta_1$.\n",
    "The relationship between $p(\\mathbf{X})$ and $\\mathbf{X}$ is not linear and because of this $\\beta_1$ does not correspond to the change in $p(\\mathbf{X})$ given one-unit increase in $\\mathbf{X}$. However, if $\\beta_1$ is positive, increasing $\\mathbf{X}$ will be associated with an increase in $p(\\mathbf{X})$ and, similarly, if $\\beta_1$ is negative, an increase in $\\mathbf{X}$ will be associated with a decrease in $p(\\mathbf{X})$. How much change will depend on the value of $\\mathbf{X}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Regression Coefficients\n",
    "\n",
    "Logistic regression uses a strategy called __maximum likelihood__ to estimate regression coefficients.\n",
    "\n",
    "Maximum likelihood plays out like so: determine estimates for $\\beta_0$ and $\\beta_1$ such that the predicted probability of $\\hat{p}(x_i)$ corresponds with the observed classes as closely as possible. Formally, this yield an equation called a __likelihood function__:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\beta_0, \\beta_1) = \\prod_{i:y_i=1}p(\\mathbf{X}_i) \\times \\prod_{j:y_j=0}(1−p(\\mathbf{X}_j)).\n",
    "$$\n",
    "\n",
    "Estimates for $\\beta_0$ and $\\beta_1$ are chosen so as to maximize this likelihood function.\n",
    "\n",
    "Linear regression’s least squares approach is actually a special case of maximum likelihood.\n",
    "\n",
    "Logistic regression measures the accuracy of coefficient estimates using a quantity called the __z-statistic__. The z-statistic is similar to the t-statistic. The z-statistic for $\\beta_1$ is represented by\n",
    "\n",
    "$$\\text{z-statistic }(\\beta_1) = \\frac{\\hat{\\beta}_1}{SE(\\hat{\\beta}_1)}$$\n",
    "\n",
    "A large z-statistic offers evidence against the null hypothesis.\n",
    "\n",
    "In logistic regression, the null hypothesis\n",
    "\n",
    "$$H_0: \\beta_1 = 0$$\n",
    "implies that\n",
    "\n",
    "$$p(\\mathbf{X}) = \\frac{e^{\\beta_0}}{1 + e^{\\beta_0}}$$\n",
    "\n",
    "and, ergo, $p(\\mathbf{X})$ does not depend on $\\mathbf{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "Once coefficients have been estimated, predictions can be made by plugging the coefficients into the model equation\n",
    "\n",
    "$$\\hat{p}(\\mathbf{X}) = \\frac{e^{\\hat{\\beta}_0 + \\hat{\\beta}_1\\mathbf{X}}}{1+e^{\\hat{\\beta}_0 + \\hat{\\beta}_1\\mathbf{X}}}.$$\n",
    "\n",
    "In general, the estimated intercept, $\\beta_0$, is of limited interest since it mainly captures the ratio of positive and negative classifications in the given data set.\n",
    "\n",
    "Similar to linear regression, __dummy variables__ can be used to accommodate qualitative predictors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
