{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression\n",
    "__Simple linear regression__ predicts a quantitative response $\\mathbf{Y}$ on the basis of a single predictor variable $\\mathbf{X}$. It assumes an approximately linear relationship between $\\mathbf{X}$ and $\\mathbf{Y}$. Formally,\n",
    "\n",
    "$$ \\mathbf{Y} \\approx \\beta_0 +\\beta_1 \\mathbf{X}$$\n",
    "\n",
    "where $\\beta_0$ represents the intercept or the value of $\\mathbf{Y}$ when $\\mathbf{X}$ is equal to 0 and $\\beta_1$ represents the slope of the line or the average amount of change in $\\mathbf{Y}$ for each one-unit increase in $\\mathbf{X}$.\n",
    "\n",
    "Together, $\\beta_0$ and $\\beta_1$ are known as the model __coefficients__ or __parameters__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating Model Coefficients\n",
    "\n",
    "Since $\\beta_0$ and $\\beta_1$ are typically unknown, it is first necessary to estimate the coefficients before making predictions. To estimate the coefficients, it is desirable to choose values for $\\beta_0$ and $\\beta_1$ such that the resulting line is as close as possible to the observed data points.\n",
    "\n",
    "There are many ways of measuring closeness. The most common method strives to minimizes the sum of the __residual__ square differences between the $i$th observed value and the $i$th predicted value.\n",
    "\n",
    "Assuming the $i$th prediction of $\\mathbf{Y}$ is described as\n",
    "\n",
    "$$\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta_1}x_i$$\n",
    "\n",
    "then the $i$th residual can be represented as\n",
    "\n",
    "$$e_i=y_i−\\hat{y}_i=y_i−\\hat{\\beta}_0−\\hat{\\beta}_1x_i.$$\n",
    "\n",
    "The __residual sum of squares__ can then be described as\n",
    "\n",
    "$$RSS = e^2_1 + e^2_2 + \\ldots + e^2_n$$\n",
    "or\n",
    "\n",
    "$$RSS=(y_1−\\hat{\\beta}_0−\\hat{\\beta}_1x_1)^2 + (y_2−\\hat{\\beta}_0−\\hat{\\beta}_1x_2)^2 + \\ldots +(y_n−\\hat{\\beta}_0−\\hat{\\beta}_1x_n)^2.$$\n",
    "\n",
    "Assuming sample means of\n",
    "\n",
    "$$\\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i$$\n",
    "and\n",
    "\n",
    "$$\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i, $$\n",
    "\n",
    "calculus can be applied to estimate the least squares coefficient estimates for linear regression to minimize the residual sum of squares like so:\n",
    "\n",
    "$$\\beta_1=\\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n(x_i - \\bar{x})^2}$$\n",
    "$$\n",
    "\\beta_0 = \\bar{y} - \\hat{\\beta}_1\\bar{x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing Coefficient Estimate Accuracy\n",
    "\n",
    "Simple linear regression represents the relationship between $\\mathbf{Y}$ and $\\mathbf{X}$ as\n",
    "\n",
    "$$\\mathbf{Y} = \\beta_0 + \\beta_1\\mathbf{Y} + \\epsilon$$\n",
    "\n",
    "where $\\beta_0$ is the intercept term, or the value of $\\mathbf{Y}$ when $\\mathbf{X} = 0$; $\\beta_1$ is the slope, or average increase in $\\mathbf{Y}$ associated with a one-unit increase in $\\mathbf{X}$; and $\\epsilon$ is the error term which acts as a catchall for what is missed by the simple model given that the true relationship likely isn’t linear, there may be other variables that affect $\\mathbf{Y}$, and/or there may be error in the observed measurements. The error term is typically assumed to be independent of $\\mathbf{X}$.\n",
    "\n",
    "The model used by simple linear regression defines the _population regression line_, which describes the best linear approximation to the true relationship between $\\mathbf{X}$ and $\\mathbf{Y}$ for the population.\n",
    "\n",
    "The coefficient estimates yielded by least squares regression characterize the __least squares line__,\n",
    "\n",
    "$$\\hat{y}_i = \\beta_0 + \\beta_1x_i.$$\n",
    "\n",
    "The difference between the population regression line and the least squares line is similar to the difference that emerges when using a sample to estimate the characteristics of a large population.\n",
    "\n",
    "In linear regression, the unknown coefficients, $\\beta_0$ and $\\beta_1$ define the population regression line, whereas the estimates of those coefficients, $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ define the least squares line.\n",
    "\n",
    "Though the parameter estimates for a given sample may overestimate or underestimate the value of a particular parameter, an __unbiased estimator__ does not systemically overestimate or underestimate the true parameter.\n",
    "\n",
    "This means that using an unbiased estimator and a large number of data sets, the values of the coefficients $\\beta_0$ and $\\beta_1$ could be determined by averaging the coefficient estimates from each of those data sets.\n",
    "\n",
    "To estimate the accuracy of a single estimated value, such as an average, it can be helpful to calculate the __standard error__ of the estimated value $\\hat{\\mu}$, which can be accomplished like so\n",
    "\n",
    "$$Var(\\hat{\\mu}) = SE(\\hat{\\mu})^2 = \\frac{\\sigma^2}{n}$$\n",
    "\n",
    "where $\\sigma$ is the standard deviation of each $y_i$.\n",
    "\n",
    "Roughly, the standard error describes the average amount that the estimate $\\hat{\\mu}$ differs from $\\mu$.\n",
    "\n",
    "The more observations, the larger $n$, the smaller the standard error.\n",
    "\n",
    "To compute the standard errors associated with $\\beta_0$ and $\\beta_1$, the following formulas can be used:\n",
    "\n",
    "$$SE(\\beta_0)^2 = \\sigma^2\\bigg[\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n(x_i - \\bar{x})^2}\\bigg]$$\n",
    "and\n",
    "\n",
    "$$SE(\\beta_1)^2 = \\frac{\\sigma^2}{\\sum_{i = 1}^n(x_i - \\bar{x})^2}$$\n",
    "\n",
    "where $\\sigma^2 = Var(\\epsilon)$ and $\\epsilon_i$ is not correlated with $\\sigma^2$.\n",
    "\n",
    "$\\sigma^2$ generally isn’t known, but can be estimated from the data. The estimate of $\\sigma$ is known as the __residual standard error__ and can be calculated with the following formula\n",
    "\n",
    "$$RSE = \\sqrt{\\frac{RSS}{(n - 2)}}$$\n",
    "\n",
    "where $RSS$ is the residual sum of squares.\n",
    "\n",
    "Standard errors can be used to compute confidence intervals and prediction intervals.\n",
    "\n",
    "A confidence interval is defined as a range of values such that there’s a certain likelihood that the range will contain the true unknown value of the parameter.\n",
    "\n",
    "For simple linear regression the 95% confidence interval for $\\beta_1$ can be approximated by\n",
    "\n",
    "$$\\hat{\\beta}_1 \\pm \\times SE(\\hat{\\beta}_1).$$\n",
    "Similarly, a confidence interval for $\\beta_0$ can be approximated as\n",
    "\n",
    "$$\\hat{\\beta}_0 \\pm 2 \\times SE(\\hat{\\beta}_0).$$\n",
    "\n",
    "The accuracy of an estimated prediction depends on whether we wish to predict an individual response, $y = f(x) + \\epsilon$ or the average response, $f(x)$.\n",
    "\n",
    "When predicting an individual response, $y = f(x) + \\epsilon$, a __prediction interval__ is used.\n",
    "\n",
    "When predicting an average response, $f(x)$, a confidence interval is used.\n",
    "\n",
    "Prediction intervals will always be wider than confidence intervals because they take into account the uncertainty associated with ϵ, the irreducible error.\n",
    "\n",
    "The standard error can also be used to perform hypothesis testing on the estimated coefficients.\n",
    "\n",
    "The most common hypothesis test involves testing the __null hypothesis__ that states\n",
    "\n",
    "$\\mathbf{H}_0$: There is no relationship between $\\mathbf{X}$ and $\\mathbf{Y}$\n",
    "versus the alternative hypothesis\n",
    "\n",
    "$\\mathbf{H}_1$: Thee is some relationship between $\\mathbf{X}$ and $\\mathbf{Y}$.\n",
    "\n",
    "In mathematical terms, the null hypothesis corresponds to testing if $\\beta_1=0$, which reduces to\n",
    "\n",
    "$$\\mathbf{Y} = \\beta_0 + \\epsilon$$\n",
    "\n",
    "which evidences that $\\mathbf{X}$ is not related to $\\mathbf{Y}$.\n",
    "\n",
    "To test the null hypothesis, it is necessary to determine whether the estimate of $\\beta_1, \\hat{\\beta}_1$, is sufficiently far from zero to provide confidence that $\\beta_1$ is non-zero.\n",
    "\n",
    "How close is close enough depends on $SE(\\hat{\\beta}_1)$. When $SE(\\hat{\\beta}_1)$ is small, then small values of $\\hat{\\beta}_1$ may provide strong evidence that $\\hat{\\beta}_1$ is not zero. Conversely, if $SE(\\hat{\\beta}_1)$ is large, then $\\hat{\\beta}_1$ will need to be large in order to reject the null hypothesis.\n",
    "\n",
    "In practice, computing a __T-statistic__, which measures the number of standard deviations that $\\hat{\\beta}_1$, is away from 0, is useful for determining if an estimate is sufficiently significant to reject the null hypothesis.\n",
    "\n",
    "A T-statistic can be computed as follows\n",
    "\n",
    "$$t = \\frac{\\hat{\\beta}_1 −0}{SE(\\hat{\\beta}_1)}$$\n",
    "\n",
    "If there is no relationship between $\\mathbf{X}$ and $\\mathbf{Y}$, it is expected that a __t-distribution__ with $n−2$ degrees of freedom should be yielded.\n",
    "\n",
    "With such a distribution, it is possible to calculate the probability of observing a value of $|t|$ or larger assuming that $\\hat{\\beta}_1 = 0 $. This probability, called the $p$-value, can indicate an association between the predictor and the response if sufficiently small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing Model Accuracy\n",
    "Once the null hypothesis has been rejected, it may be desirable to quantify to what extent the model fits the data. The quality of a linear regression model is typically assessed using __residual standard error (RSE)__ and the $\\mathbf{R}^2$ __statistic__.\n",
    "\n",
    "The residual standard error is an estimate of the standard deviation of $\\epsilon$, the irreducible error.\n",
    "\n",
    "In rough terms, the residual standard error is the average amount by which the response will deviate from the true regression line.\n",
    "\n",
    "For linear regression, the residual standard error can be computed as\n",
    "\n",
    "$$RSE = \\sqrt{\\frac{1}{n-2}RSS}=\\sqrt{\\frac{1}{n-2}\\sum_{i=1}^n(y_i - \\hat{y}_i)^ 2}$$\n",
    "\n",
    "The residual standard error is a measure of the lack of fit of the model to the data. When the values of $y_i \\approx \\hat{y}_i$, the $RSE$ will be small and the model will fit the data well. Conversely, if $y_i \\neq \\hat{y}_i$ for some values, the $RSE$ may be large, indicating that the model doesn’t fit the data well.\n",
    "\n",
    "The $RSE$ provides an absolute measure of the lack of fit of the model in the units of Y. This can make it difficult to know what constitutes a good RSE value.\n",
    "\n",
    "The $R^2 statistic$ is an alternative measure of fit that takes the form of a proportion. The $R^2$ statistic captures the proportion of variance explained as a value between 0 and 1, independent of the unit of $\\mathbf{Y}$.\n",
    "\n",
    "To calculate the $R^2$ statistic, the following formula may be used\n",
    "\n",
    "$$\n",
    "R^2 = \\frac{TSS−RSS}{TSS} = 1 − \\frac{RSS}{TSS}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$RSS = \\sum_{i=1}^n(y_i − \\hat{y}_i)^2$$\n",
    "\n",
    "and\n",
    "\n",
    "$$TSS = \\sum_{i=1}^n(y_i − \\bar{y}_i)^2.$$\n",
    "\n",
    "The __total sum of squares__, TSS, measures the total variance in the response $\\mathbf{Y}$. The TSS can be thought of as the total variability in the response before applying linear regression. Conversely, the residual sum of squares, RSS, measures the amount of variability left after performing the regression.\n",
    "\n",
    "Ergo, $TSS−RSS$ measures the amount of variability in the response that is explained by the model. $R^2$ measures the proportion of variability in $\\mathbf{Y}$ that can be explained by $\\mathbf{X}$. An $R^2$ statistic close to 1 indicates that a large portion of the variability in the response is explained by the model. An $R^2$ value near 0 indicates that the model accounted for very little of the variability of the model.\n",
    "\n",
    "An $R^2$ value near 0 may occur because the linear model is wrong and/or because the inherent $\\sigma^2$ is high.\n",
    "\n",
    "$R^2$ has an advantage over $RSE$ since it will always yield a value between 0 and 1, but it can still be tough to know what a good $R^2$ value is. Frequently, what constitutes a good $R^2$ value depends on the application and what is known about the problem.\n",
    "\n",
    "The $R^2$ statistic is a measure of the linear relationship between $\\mathbf{X}$ and $\\mathbf{Y}$.\n",
    "\n",
    "__Correlation__ is another measure of the linear relationship between $\\mathbf{X}$ and $\\mathbf{Y}$.\n",
    "\n",
    "Correlation of can be calculated as\n",
    "\n",
    "$$Cor(\\mathbf{X}, \\mathbf{Y}) = \\frac{\\sum_{i=1}^n(x_i − \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n(x_i − \\bar{x})^2}\\sqrt{\\sum_{i=1}^n(y_i − \\bar{y})^2}}$$\n",
    "\n",
    "This suggests that $r = Cor(\\mathbf{X}, \\mathbf{Y})$ could be used instead of $R^2$ to assess the fit of the linear model, however for simple linear regression it can be shown that $R^2 = r^2$. More concisely, for simple linear regression, the squared correlation and the $R^2$ statistic are equivalent. Though this is the case for simple linear regression, correlation does not extend to multiple linear regression since correlation quantifies the association between a single pair of variables. The $R^2$ statistic can, however, be applied to multiple regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Regression\n",
    "The __multiple linear regression__ model takes the form of\n",
    "\n",
    "$$\\mathbf{Y} = \\beta_0 + \\beta_1\\mathbf{X}_1 + \\beta_2\\mathbf{X}_2 + \\ldots + \\beta_p\\mathbf{X}_p + \\epsilon$$\n",
    "\n",
    "Multiple linear regression extends simple linear regression to accommodate multiple predictors.\n",
    "\n",
    "$\\mathbf{X}_j$ represents the $j^{th}$ predictor and $\\beta_j$ represents the average effect of a one-unit increase in $\\mathbf{X}_j$ on $\\mathbf{Y}$, holding all other predictors fixed.\n",
    "\n",
    "### Estimating Multiple Regression Coefficients\n",
    "Because the coefficients $\\beta_0, \\beta_1, \\ldots, \\beta_p$ are unknown, it is necessary to estimate their values. Given estimates of $\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p$ estimates can be made using the formula below\n",
    "\n",
    "$$\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\hat{\\beta}_2x_2+ \\ldots + \\hat{\\beta}_px_p$$\n",
    "\n",
    "The parameters $\\beta_0, \\beta_1, \\ldots, \\beta_p$ can be estimated using the same least squares strategy as was employed for simple linear regression. Values are chosen for the parameters $\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p$ such that the residual sum of squares is minimized\n",
    "\n",
    "$$ RSS = \\sum_{i=1}^n(y_i − \\hat{y}_i)^2 = \\sum_{i=1}^n(y_i −\\hat{\\beta}_0 - \\hat{\\beta}_1x_1 - \\hat{\\beta}_2x_2 - \\ldots - \\hat{\\beta}_px_p)^2$$\n",
    "Estimating the values of these parameters is best achieved with matrix algebra.\n",
    "\n",
    "### Assessing Multiple Regression Coefficient Accuracy\n",
    "Once estimates have been derived, it is next appropriate to test the null hypothesis\n",
    "\n",
    "$$H_0: \\beta_1 = \\beta_2 = \\ldots = \\beta_p = 0$$\n",
    "versus the alternative hypothesis\n",
    "\n",
    "$$H_a: \\text{at least one of } \\beta_j \\neq 0.$$\n",
    "\n",
    "The __F-statistic__ can be used to determine which hypothesis holds true.\n",
    "\n",
    "The F-statistic can be computed as\n",
    "\n",
    "$$F = \\frac{(TSS−RSS)/p}{RSS/(n−p−1)} = \\frac{\\frac{TSS−RSS}{p}}{\\frac{RSS}{n−p−1}}$$\n",
    "where, again,\n",
    "\n",
    "$$TSS = \\sum_{i = 1}^n(y_i − \\bar{y}_i)^2$$\n",
    "and\n",
    "\n",
    "$$RSS = \\sum_{i = 1}^n(y_i − \\hat{y}_i)^2$$\n",
    "\n",
    "If the assumptions of the linear model, represented by the alternative hypothesis, are true it can be shown that\n",
    "\n",
    "$$E\\bigg\\{\\frac{RSS}{n−p−1}\\bigg\\} = \\sigma^2$$\n",
    "\n",
    "Conversely, if the null hypothesis is true, it can be shown that\n",
    "\n",
    "$$E\\bigg\\{\\frac{TSS−RSS}{p}\\bigg\\} = \\sigma^2$$\n",
    "\n",
    "This means that when there is no relationship between the response and the predictors the F-statisitic takes on a value close to 1.\n",
    "\n",
    "Conversely, if the alternative hypothesis is true, then the F-statistic will take on a value greater than 1.\n",
    "\n",
    "When $n$ is large, an F-statistic only slightly greater than 1 may provide evidence against the null hypothesis. If $n$ is small, a large F-statistic is needed to reject the null hypothesis.\n",
    "\n",
    "When the null hypothesis is true and the errors $\\epsilon_i$ have a __normal distribution__, the F-statistic follows and __F-distribution__. Using the F-distribution, it is possible to figure out a p-value for the given $n, p,$ and F-statistic. Based on the obtained p-value, the validity of the null hypothesis can be determined.\n",
    "\n",
    "It is sometimes desirable to test that a particular subset of $q$ coefficients are 0. This equates to a null hypothesis of\n",
    "\n",
    "$$H_0: \\beta_{p−q+1} = \\beta_{p−q+2} = \\ldots = \\beta-p = 0.$$\n",
    "\n",
    "Supposing that the residual sum of squares for such a model is RSS0 then the F-statistic could be calculated as\n",
    "\n",
    "$$F= \\frac{(RSS0−RSS)/q}{RSS/(n−p−1)} = \\frac{\\frac{RSS_0−RSS}{q}}{\\frac{RSS}{n−p−1}}.$$\n",
    "\n",
    "Even in the presence of $p$-values for each individual variable, it is still important to consider the overall F-statistic because there is a reasonably high likelihood that a variable with a small p-value will occur just by chance, even in the absence of any true association between the predictors and the response.\n",
    "\n",
    "In contrast, the F-statistic does not suffer from this problem because it adjusts for the number of predictors. The F-statistic is not infallible and when the null hypothesis is true the F-statistic can still result in p-values below 0.05 about 5% of the time regardless of the number of predictors or the number of observations.\n",
    "\n",
    "The F-statistic works best when p is relatively small or when p is relatively small compared to $n$.\n",
    "\n",
    "When $p$ is greater than $n$, multiple linear regression using least squares will not work, and similarly, the F-statistic cannot be used either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Important Variables\n",
    "Once it has been established that at least one of the predictors is associated with the response, the question remains, which of the predictors is related to the response? The process of removing extraneous predictors that don’t relate to the response is called __variable selection__.\n",
    "\n",
    "Ideally, the process of variable selection would involve testing many different models, each with a different subset of the predictors, then selecting the best model of the bunch, with the meaning of “best” being derived from various statistical methods.\n",
    "\n",
    "Regrettably, there are a total of $2^p$ models that contain subsets of $p$ predictors. Because of this, an efficient and automated means of choosing a smaller subset of models is needed. There are a number of statistical approaches to limiting the range of possible models.\n",
    "\n",
    "__Forward selection__ begins with a __null model__, a model that has an intercept but no predictors, and attempts $p$ simple linear regressions, keeping whichever predictor results in the lowest residual sum of squares. In this fashion, the predictor yielding the lowest RSS is added to the model one-by-one until some halting condition is met. Forward selection is a greedy process and it may include extraneous variables.\n",
    "\n",
    "__Backward selection__ begins with a model that includes all the predictors and proceeds by removing the variable with the highest p-value each iteration until some stopping condition is met. Backwards selection cannot be used when $p > n$.\n",
    "\n",
    "__Mixed selection__ begins with a null model, like forward selection, repeatedly adding whichever predictor yields the best fit. As more predictors are added, the p-values become larger. When this happens, if the p-value for one of the variables exceeds a certain threshold, that variable is removed from the model. The selection process continues in this forward and backward manner until all the variables in the model have sufficiently low $p$-values and all the predictors excluded from the model would result in a high $p$-value if added to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing Multiple Regression Model Fit\n",
    "\n",
    "While in simple linear regression the $R^2$, the fraction of variance explained, is equal to $Cor(\\mathbf{X}, \\mathbf{Y})$, in multiple linear regression, R2 is equal to Cor(Y,Y^)2. In other words, R2 is equal to the square of the correlation between the response and the fitted linear model. In fact, the fitted linear model maximizes this correlation among all possible linear models.\n",
    "\n",
    "An $R^2$ close to 1 indicates that the model explains a large portion of the variance in the response variable. However, it should be noted that $R^2$ will always increase when more variables are added to the model, even when those variables are only weakly related to the response. This happens because adding another variable to the least squares equation will always yield a closer fit to the training data, though it won’t necessarily yield a closer fit to the test data.\n",
    "\n",
    "__Residual standard error__, RSE, can also be used to assess the fit of a multiple linear regression model. In general, RSE can be calculated as\n",
    "\n",
    "$$RSE = \\sqrt{\\frac{RSS}{n−p−1}}$$\n",
    "\n",
    "which simplifies to the following for simple linear regression\n",
    "\n",
    "$$RSE = \\sqrt{\\frac{RSS}{n−2}}.$$\n",
    "\n",
    "Given the definition of RSE for multiple linear regression, it can be seen that models with more variables can have a higher RSE if the decrease in RSS is small relative to the increase in $p$.\n",
    "\n",
    "In addition to $R^2$ and RSE, it can also be useful to plot the data to verify the model.\n",
    "\n",
    "Once coefficients have been estimated, making predictions is a simple as plugging the coefficients and predictor values into the multiple linear model\n",
    "\n",
    "$$\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\hat{\\beta}_2x_2+ \\ldots + \\hat{\\beta}_px_p$$\n",
    "\n",
    "However, it should be noted that these predictions will be subject to three types of uncertainty.\n",
    "\n",
    "1. The coefficient estimates, $\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p$, are only estimates of the actual coefficients $\\beta_0, \\beta_1, \\ldots, \\beta_p$. That is to say, the least squares plane is only an estimate of the true population regression plane. The error introduced by this inaccuracy is reducible error and a confidence interval can be computed to determine how close $\\hat{y} is to $f(\\mathbf{X})$.\n",
    "\n",
    "2. Assuming a linear model for $f(\\mathbf{X})$ is almost always an approximation of reality, which means additional reducible error is introduced due to model bias. A linear model often models the best linear approximation of the true, non-linear surface.\n",
    "\n",
    "3. Even in the case where $f(\\mathbf{X})$ and the true values of the coefficients,$\\beta_0, \\beta_1, \\ldots, \\beta_p$ are known, the response value cannot be predicted exactly because of the random, irreducible error ϵ, in the model. How much $\\hat{\\mathbf{Y}}$ will tend to vary from $\\mathbf{Y}$ can be determined using prediction intervals.\n",
    "\n",
    "Prediction intervals will always be wider than confidence intervals because they incorporate both the error in the estimate of $f(\\mathbf{X})$, the reducible error, and the variation in how each point differs from the population regression plane, the irreducible error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative predictors\n",
    "Linear regression can also accommodate qualitative variables.\n",
    "\n",
    "When a qualitative predictor or factor has only two possible values or levels, it can be incorporated into the model my introducing an __indicator variable__ or __dummy variable__ that takes on only two numerical values.\n",
    "\n",
    "For example, using a coding like\n",
    "\n",
    "$$\n",
    "\\mathbf{X}_i = \n",
    "\\begin{cases}\n",
    "    1 & \\text{if } p_i = \\text{class A}\\\\\n",
    "    0 & \\text{if } p_i = \\text{class B}\n",
    "\\end{cases}\n",
    "$$\n",
    "yields a regression equation like\n",
    "\n",
    "$$\n",
    "y_i = \\beta_0 + \\beta_1\\mathbf{X}_1 + \\epsilon_i = \n",
    "\\begin{cases}\\beta_0 + \\beta_1 + \\epsilon_i \\text{if }p_i = \\text{class A}\\\\\n",
    "\\beta_0 + \\epsilon_i \\text{if } p_i = \\text{class B}\n",
    "\\end{cases}\n",
    "$$\n",
    "Given such a coding, $\\beta_1$ represents the average difference in $\\mathbf{X}_1$ between classes A and B.\n",
    "\n",
    "Alternatively, a dummy variable like the following could be used\n",
    "\n",
    "$$\n",
    "\\mathbf{X}_i = \n",
    "\\begin{cases}\n",
    "    1 & \\text{if } p_i = \\text{class A}\\\\\n",
    "    -1 & \\text{if } p_i = \\text{class B}\n",
    "\\end{cases}\n",
    "$$\n",
    "which results in a regression model like\n",
    "\n",
    "$$\n",
    "y_i = \\beta_0 + \\beta_1\\mathbf{X}_1 + \\epsilon_i = \n",
    "\\begin{cases}\\beta_0 + \\beta_1 + \\epsilon_i \\text{if }p_i = \\text{class A}\\\\\n",
    "\\beta_0 - \\beta_1 \\epsilon_i \\text{if } p_i = \\text{class B}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "In which case, $\\beta_0$ represents the overall average and $\\beta_1$ is the amount class A is above the average and class B below the average.\n",
    "\n",
    "Regardless of the coding scheme, the predictions will be equivalent. The only difference is the way the coefficients are interpreted.\n",
    "\n",
    "When a qualitative predictor takes on more than two values, a single dummy variable cannot represent all possible values. Instead, multiple dummy variables can be used. The number of variables required will always be one less than the number of values that the predictor can take on.\n",
    "\n",
    "For example, with a predictor that can take on three values, the following coding could be used\n",
    "\n",
    "$$\n",
    "\\mathbf{X}_{i1} = \n",
    "\\begin{cases}\n",
    "    1 & \\text{ if } p_i = \\text{class A}\\\\\n",
    "    0 & \\text{ if } p_i \\neq \\text{class A}\n",
    "\\end{cases}\n",
    "$$\n",
    "$$\n",
    "\\mathbf{X}_{i2} = \n",
    "\\begin{cases}\n",
    "    1 & \\text{ if } p_i = \\text{class B}\\\\\n",
    "    -1 & \\text{ if } p_i \\neq \\text{class B}\n",
    "\\end{cases}\n",
    "$$\n",
    "$$\n",
    "y_i = \\beta_0 + \\beta_1\\mathbf{X}_1 + \\beta_2\\mathbf{X}_2 + \\epsilon_i = \n",
    "\\begin{cases}\\beta_0 + \\beta_1 + \\epsilon_i \\text{ if }p_i = \\text{class A}\\\\\n",
    "\\beta_0 + \\beta_2 + \\epsilon_i \\text{ if } p_i = \\text{class B} \\\\\n",
    "\\beta_0 + \\epsilon_i \\text{ if } p_i = \\text{class C}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "With such a coding, $\\beta_0$ can be interpreted as the average response for class C. $\\beta_1$ can be interpreted as the average difference in response between classes A and C. Finally, $\\beta_2$ can be interpreted as the average difference in response between classes B and C.\n",
    "\n",
    "The case where $\\beta_1$ and $\\beta_2$ are both zero, the level with no dummy variable, is known as the baseline.\n",
    "\n",
    "Using dummy variables allows for easily mixing quantitative and qualitative predictors.\n",
    "\n",
    "There are many ways to encode dummy variables. Each approach yields equivalent model fits, but results in different coefficients and different interpretations that highlight different contrasts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extending the Linear Model\n",
    "Though linear regression provides interpretable results, it makes several highly restrictive assumptions that are often violated in practice. One assumption is that the relationship between the predictors and the response is additive. Another assumption is that the relationship between the predictors and the response is linear.\n",
    "\n",
    "The additive assumption implies that the effect of changes in a predictor $\\mathbf{X}_j$ on the response $\\mathbf{Y}$ is independent of the values of the other predictors.\n",
    "\n",
    "The linear assumption implies that the change in the response Y due to a one-unit change in $\\mathbf{X}_j$ is constant regardless of the value of $\\mathbf{X}_j$.\n",
    "The additive assumption ignores the possibility of an interaction between predictors. One way to account for an interaction effect is to include an additional predictor, called an __interaction term__, that computes the product of the associated predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Predictor Interaction\n",
    "A simple linear regression model account for interaction between the predictors would look like\n",
    "\n",
    "$$\n",
    "\\mathbf{Y} = \\beta_0 + \\beta_1\\mathbf{X}_1 + \\beta_2\\mathbf{X}_2 + \\beta_3\\mathbf{X}_1\\mathbf{X}_2 + \\epsilon_i\n",
    "$$\n",
    "\n",
    "$\\beta_3$ can be interpreted as the increase in effectiveness of $\\beta_1$ given a one-unit increase in $\\beta_2$ and vice-versa.\n",
    "\n",
    "It is sometimes possible for an interaction term to have a very small p-value while the associated main effects, $\\mathbf{X}_1, \\mathbf{X}_2$,etc., do not. Even in such a scenario the main effects should still be included in the model due to the hierarchical principle.\n",
    "\n",
    "The hierarchical principle states that, when an interaction term is included in the model, the main effects should also be included, even if the p-values associated with their coefficients are not significant. The reason for this is that $\\mathbf{X}_1\\mathbf{X}_2$ is often correlated with $\\mathbf{X}_1$ and $\\mathbf{X}_2$ and removing them tends to change the meaning of the interaction.\n",
    "\n",
    "If $\\mathbf{X}_1\\mathbf{X}_2$ is related to the response, then whether or not the coefficient estimates of $\\mathbf{X}_1$ or $\\mathbf{X}_2$ are exactly zero is of limited interest.\n",
    "\n",
    "Interaction terms can also model a relationship between a quantitative predictor and a qualitative predictor.\n",
    "\n",
    "In the case of simple linear regression with a qualitative variable and without an interaction term, the model takes the form\n",
    "\n",
    "$$\n",
    "y_i = \\beta_0 + \\beta_1\\mathbf{X}_1 + \\begin{cases} \\beta_2 & \\text{ if } p_i = \\text{ class A} \\\\\n",
    "0 & \\text{ if } p_i \\neq \\text{ class A}\n",
    "\\end{cases}\n",
    "$$\n",
    "with the addition of an interaction term, the model takes the form\n",
    "\n",
    "$$\n",
    "y_i = \\beta_0 + \\beta_1\\mathbf{X}_1 + \\begin{cases} \\beta_2 + \\beta_3\\mathbf{X}_1 & \\text{ if } p_i = \\text{ class A} \\\\\n",
    "0 & \\text{ if } p_i \\neq \\text{ class A}\n",
    "\\end{cases}\n",
    "$$\n",
    "which is equivalent to\n",
    "\n",
    "$$\n",
    "y_i = \\begin{cases} (\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3)\\mathbf{X}_1 & \\text{ if } p_i = \\text{ class A} \\\\\n",
    "\\beta_0 + \\beta_1\\mathbf{X}_1 & \\text{ if } p_1 \\neq \\text{ class A}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Non-Linear Relationships\n",
    "\n",
    "To mitigate the effects of the linear assumption it is possible to accommodate non-linear relationships by incorporating polynomial functions of the predictors in the regression model.\n",
    "\n",
    "For example, in a scenario where a quadratic relationship seems likely, the following model could be used\n",
    "\n",
    "$$\\mathbf{Y}_i = \\beta_0 + \\beta_1\\mathbf{X}_1 + \\beta_2\\mathbf{X}^2_1 + \\epsilon$$\n",
    "This extension of the linear model to accommodate non-linear relationships is called __polynomial regression__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Problems with Linear Regression\n",
    "1. Non-linearity of the response-predictor relationship\n",
    "2. Correlation of error terms\n",
    "3. Non-constant variance of error terms\n",
    "4. Outliers\n",
    "5. High-leverage points\n",
    "6. Collinearity\n",
    "\n",
    "#### 1. Non-linearity of the response-predictor relationship\n",
    "If the true relationship between the response and predictors is far from linear, then virtually all conclusions that can be drawn from the model are suspect and prediction accuracy can be significantly reduced.\n",
    "\n",
    "__Residual plots__ are a useful graphical tool for identifying non-linearity. For simple linear regression this consists of graphing the residuals, $e_i = y_i − \\hat{y}_i$ versus the predicted or fitted values of $\\hat{y}_i$.\n",
    "\n",
    "If a residual plot indicates non-linearity in the model, then a simple approach is to use non-linear transformations of the predictors, such as log $x$, $\\sqrt{x}$, or $x^2$, in the regression model.\n",
    "\n",
    "![plot](\"residual-plot.jpg\")\n",
    "\n",
    "The example residual plots above suggest that a quadratic fit may be more appropriate for the model under scrutiny."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Correlation of error terms\n",
    "An important assumption of linear regression is that the error terms, $\\epsilon_1, \\epsilon_2, \\ldots, \\epsilon_n,$ are uncorrelated. Because the estimated regression coefficients are calculated based on the assumption that the error terms are uncorrelated, if the error terms are correlated it will result in incorrect standard error values that will tend to underestimate the true standard error. This will result in prediction intervals and confidence intervals that are narrower than they should be. In addition, p-values associated with the model will be lower than they should be. In other words, correlated error terms can make a model appear to be stronger than it really is.\n",
    "\n",
    "Correlations in error terms can be the result of time series data, unexpected observation relationships, and other environmental factors. Observations that are obtained at adjacent time points will often have positively correlated errors. Good experiment design is also a crucial factor in limiting correlated error terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Non-constant variance of error terms\n",
    "Linear regression also assumes that the error terms have a constant variance,\n",
    "\n",
    "$$\\text{Var}(\\epsilon_1) = \\sigma^2.$$\n",
    "\n",
    "Standard errors, confidence intervals, and hypothesis testing all depend on this assumption.\n",
    "\n",
    "Residual plots can help identify non-constant variances in the error, or __heteroscedasticity__, if a funnel shape is present.\n",
    "![plot2](\"residual-funnel.jpg\")\n",
    "\n",
    "One way to address this problem is to transform the response $\\mathbf{Y}$ using a concave function such as log $\\mathbf{Y}$ or $\\sqrt{\\mathbf{Y}}$. This results in a greater amount of shrinkage of the larger responses, leading to a reduction in heteroscedasticity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Outliers\n",
    "An __outlier__ is a point for which $y_i$ is far from the value predicted by the model.\n",
    "\n",
    "Excluding outliers can result in improved residual standard error and improved $R^2$ values, usually with negligible impact to the least squares fit.\n",
    "\n",
    "Residual plots can help identify outliers, though it can be difficult to know how big a residual needs to be before considering a point an outlier. To address this, it can be useful to plot the __studentized residuals__ instead of the normal residuals. Studentized residuals are computed by dividing each residual, $e_i$, by its estimated standard error. Observations whose studentized residual is greater than $|3|$ are possible outliers.\n",
    "\n",
    "Outliers should only be removed when confident that the outliers are due to a recording or data collection error since outliers may otherwise indicate a missing predictor or other deficiency in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. High-Leverage Points\n",
    "While outliers relate to observations for which the response yi is unusual given the predictor $x_i$, in contrast, observations with __high leverage__ are those that have an unusual value for the predictor $x_i$ for the given response $y_i$.\n",
    "\n",
    "High leverage observations tend to have a sizable impact on the estimated regression line and as a result, removing them can yield improvements in model fit.\n",
    "\n",
    "For simple linear regression, high leverage observations can be identified as those for which the predictor value is outside the normal range. With multiple regression, it is possible to have an observation for which each individual predictor’s values are well within the expected range, but that is unusual in terms of the combination of the full set of predictors.\n",
    "\n",
    "To qualify an observation’s leverage, the leverage statistic can be computed.\n",
    "\n",
    "A large leverage statistic indicates an observation with high leverage.\n",
    "\n",
    "For simple linear regression, the leverage statistic can be computed as\n",
    "\n",
    "$$h_i = \\frac{1}{n}+ \\frac{(x_i − \\bar{x})^2}{\\sum^n_{j=1}(x_j − \\bar{x})^2}.$$\n",
    "\n",
    "The leverage statistic always falls between $\\frac{1}{n}$ and 1 and the average leverage is always equal to $\\frac{p+1}{n}$. So, if an observation has a leverage statistic greatly exceeds $\\frac{p+1}{n}$ then it may be evidence that the corresponding point has high leverage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Collinearity\n",
    "__Collinearity__ refers to the situation in which two or more predictor variables are closely related to one another.\n",
    "\n",
    "Collinearity can pose problems for linear regression because it can make it hard to determine the individual impact of collinear predictors on the response.\n",
    "\n",
    "Collinearity reduces the accuracy of the regression coefficient estimates, which in turn causes the standard error of $\\beta_j$ to grow. Since the T-statistic for each predictor is calculated by dividing $\\beta_j$ by its standard error, collinearity results in a decline in the true T-statistic. This may cause it to appear that $\\beta_j$ and $x_j$ are related to the response when they are not. As such, collinearity reduces the effectiveness of the null hypothesis. Because of all this, it is important to address possible collinearity problems when fitting the model.\n",
    "\n",
    "One way to detect collinearity is to generate a correlation matrix of the predictors. Any element in the matrix with a large absolute value indicates highly correlated predictors. This is not always sufficient though, as it is possible for collinearity to exist between three or more variables even if no pair of variables have high correlation. This scenario is known as multicollinearity.\n",
    "\n",
    "__Multicollinearity__ can be detected by computing the __variance inflation factor__. The variance inflation factor is the ratio of the variance of $\\hat{\\beta}_j$ when fitting the full model divided by the variance of $\\hat{\\beta}_j$ if fit on its own. The smallest possible VIF value is 1.0, which indicates no collinearity whatsoever. In practice, there is typically a small amount of collinearity among predictors. As a general rule of thumb, VIF values that exceed 5 or 10 indicate a problematic amount of collinearity.\n",
    "\n",
    "The variance inflation factor for each variable can be computed using the formula\n",
    "\n",
    "$$VIF(\\hat{\\beta}_j) = \\frac{1}{1 − R^2_{x_j|x_{−j}}}$$\n",
    "\n",
    "where $R_{x_j|x_{−j}}$ is the $R^2$ from a regression of $\\mathbf{X}_j$ onto all of the other predictors. If $R_{x_j|x_{−j}}$ is close to one, the VIF will be large and collinearity is present.\n",
    "\n",
    "One way to handle collinearity is to drop one of the problematic variables. This usually doesn’t compromise the fit of the regression as the collinearity implies that the information that the predictor provides about the response is abundant.\n",
    "\n",
    "A second means of handling collinearity is to combine the collinear predictors together into a single predictor by some kind of transformation such as an average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametric Methods Versus Non-Parametric Methods\n",
    "A non-parametric method akin to linear regression is __k-nearest neighbors regression__ which is closely related to the k-nearest neighbors classifier.\n",
    "\n",
    "Given a value for $K$ and a prediction point $x_0$, k-nearest neighbors regression first identifies the $K$ observations that are closest to $x_0$, represented by $N_0$. f(x0) is then estimated using the average of N0i like so\n",
    "\n",
    "$$\\hat{f}(x_0) = \\frac{1}{k}\\sum_{x_i \\in N_0} y_i$$\n",
    "A parametric approach will outperform a non-parametric approach if the parametric form is close to the true form of $f(\\mathbf{X})$.\n",
    "\n",
    "The choice of a parametric approach versus a non-parametric approach will depend largely on the bias-variance trade-off and the shape of the function f(X).\n",
    "When the true relationship is linear, it is difficult for a non-parametric approach to compete with linear regression because the non-parametric approach incurs a cost in variance that is not offset by a reduction in bias. Additionally, in higher dimensions, K-nearest neighbors regression often performs worse than linear regression. This is often due to combining too small an $n$ with too large a $p$, resulting in a given observation having no nearby neighbors. This is often called the __curse of dimensionality__. In other words, the $K$ observations nearest to an observation may be far away from $x_0$ in a $p$-dimensional space when $p$ is large, leading to a poor prediction of $f(x_0)$ and a poor K-nearest neighbors regression fit.\n",
    "\n",
    "As a general rule, parametric models will tend to outperform non-parametric models when there are only a small number of observations per predictor."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
