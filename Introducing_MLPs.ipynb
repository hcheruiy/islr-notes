{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introducing MLPs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPXC2hSOhSzrfG62j0/zoof"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytpfJqBiiVfm",
        "colab_type": "text"
      },
      "source": [
        "# Introducing Advanced Deep Learning with Keras\n",
        "\n",
        "This notebook introduces the three architectures of artificial neural networks: __MLPs__, __CNNs__, and __RNNs__. They are the core building blocks of most advanced deep learning topics such as Autoencoders and GANs. We'll implement the models using the Keras library, and introduce important deep learning concepts including optimization, regularization, and loss functions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SC_ZGZ7hFtN",
        "colab_type": "text"
      },
      "source": [
        "## Keras Library\n",
        "TensorFlow, a popular open source deep learning library,uses Keras as a high-level API to its library. We'll start by introducing how to use the __Keras Sequential API__."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qnv09trykS7b",
        "colab_type": "text"
      },
      "source": [
        "## MNIST Dataset\n",
        "\n",
        "MNIST is a collection of handwritten digits ranging from the number 0 to 9. It has a training set of 60,000 images, and 10,000 test images that are classified into corresponding categories or labels. In some literature, the term __target__ or __ground truth__ is also used to refer to the __label__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUfiuqBUk__a",
        "colab_type": "code",
        "outputId": "656c2fed-97cc-4b80-9364-5a082641e3f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# accessing the MNIST Dataset\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# load dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoQ9QP1loz82",
        "colab_type": "code",
        "outputId": "4a0826c4-6654-470b-f753-61d703599d68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# count the number of unique test labels\n",
        "unique, counts = np.unique(y_train, return_counts = True)\n",
        "print(\"Train labels: \", dict(zip(unique, counts)))\n",
        "\n",
        "# count the number of unique test labels\n",
        "unique, counts = np.unique(y_test, return_counts = True)\n",
        "print(\"Test labels: \", dict(zip(unique, counts)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train labels:  {0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949}\n",
            "Test labels:  {0: 980, 1: 1135, 2: 1032, 3: 1010, 4: 982, 5: 892, 6: 958, 7: 1028, 8: 974, 9: 1009}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liNej5XUo6Kt",
        "colab_type": "code",
        "outputId": "8e6eb4d8-dcd2-443f-98f1-d293263dcd0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "# sample 25 mnist digits from train dataset\n",
        "indexes = np.random.randint(0, X_train.shape[0], size = 25)\n",
        "images = X_train[indexes]\n",
        "labels = y_train[indexes]\n",
        "\n",
        "# plot the 25 mnist digits\n",
        "plt.figure(figsize = (5, 5))\n",
        "for i in range(len(indexes)):\n",
        "  plt.subplot(5, 5, i + 1)\n",
        "  image = images[i]\n",
        "  plt.imshow(image, cmap = 'gray')\n",
        "  plt.axis('off')\n",
        "\n",
        "plt.show();"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAASQAAAEeCAYAAAAjGGgnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 360x360 with 25 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idjBVE21neAe",
        "colab_type": "text"
      },
      "source": [
        "## MNIST digits classifier model\n",
        "The MLP model for MNIST classification. The fully connected model in the figure below shows how the output of the perceptron is computed from inputs as a function of weights, $w_i$ and bias, $b_n$ for the $n^{th$}$ unit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZROc9nAkhOOg",
        "colab_type": "text"
      },
      "source": [
        "## Multilayer Perceptrons (MLPs)\n",
        "\n",
        "__MLPs__ are a fully connected network, often referred to as either deep feedforward networks or simply feed forward neural networks. Let's implement it in Keras\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoQCuVpanlqr",
        "colab_type": "code",
        "outputId": "c7da1d37-b663-418b-c3aa-87c24b52ee99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "# import libraries\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# load mnist dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# compute the number of labels\n",
        "num_labels = len(np.unique(y_train))\n",
        "\n",
        "# convert to one-hot vector\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# image dimensions\n",
        "image_size = X_train.shape[1]\n",
        "input_size = image_size * image_size\n",
        "\n",
        "# resize and normalize\n",
        "X_train = np.reshape(X_train, [-1, input_size])\n",
        "X_train = X_train.astype('float32') / 255\n",
        "X_test = np.reshape(X_test, [-1, input_size])\n",
        "X_test = X_test.astype('float32') / 255\n",
        "\n",
        "# network parameters\n",
        "batch_size = 128\n",
        "hidden_units = 256\n",
        "dropout = 0.45\n",
        "\n",
        "# model is 3-Layer MLP with ReLU and dropout after each layer\n",
        "model = Sequential()\n",
        "model.add(Dense(hidden_units, input_dim = input_size))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(hidden_units))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(num_labels))\n",
        "\n",
        "# this is the output for one-hot vector\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "plot_model(model, to_file = 'mlp-mnist.png', show_shapes = True)\n",
        "\n",
        "# loss function for one-hot vector\n",
        "# use adam optimizer\n",
        "# accuracy is a good metric for classification tasks\n",
        "model.compile(loss = 'categorical_crossentropy', \n",
        "              optimizer = 'adam',\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "# train the network\n",
        "model.fit(X_train, y_train, epochs = 20, batch_size = batch_size)\n",
        "\n",
        "# validate the model on test dataset to determine generalization\n",
        "loss, acc = model.evaluate(X_test, y_test, batch_size = batch_size)\n",
        "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 256)               200960    \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 269,322\n",
            "Trainable params: 269,322\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 3s 50us/sample - loss: 0.4166 - acc: 0.8718\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 1s 24us/sample - loss: 0.1917 - acc: 0.9430\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 1s 23us/sample - loss: 0.1517 - acc: 0.9540\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 1s 23us/sample - loss: 0.1287 - acc: 0.9610\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 1s 23us/sample - loss: 0.1127 - acc: 0.9659\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 2s 25us/sample - loss: 0.0997 - acc: 0.9690\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 1s 23us/sample - loss: 0.0948 - acc: 0.9707\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 1s 23us/sample - loss: 0.0886 - acc: 0.9724\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 1s 23us/sample - loss: 0.0817 - acc: 0.9748\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 1s 24us/sample - loss: 0.0771 - acc: 0.9760\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 1s 23us/sample - loss: 0.0747 - acc: 0.9768\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 1s 23us/sample - loss: 0.0689 - acc: 0.9784\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 1s 23us/sample - loss: 0.0669 - acc: 0.9785\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 1s 23us/sample - loss: 0.0659 - acc: 0.9790\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 1s 22us/sample - loss: 0.0621 - acc: 0.9802\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 1s 24us/sample - loss: 0.0604 - acc: 0.9812\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 1s 23us/sample - loss: 0.0570 - acc: 0.9812\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 1s 23us/sample - loss: 0.0555 - acc: 0.9817\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 1s 23us/sample - loss: 0.0551 - acc: 0.9823\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 1s 22us/sample - loss: 0.0540 - acc: 0.9826\n",
            "10000/10000 [==============================] - 0s 18us/sample - loss: 0.0669 - acc: 0.9827\n",
            "\n",
            "Test accuracy: 98.3%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOXOS4t7plxl",
        "colab_type": "text"
      },
      "source": [
        "## Explanation\n",
        "The above model is made of three MLP layers. In Keras, an MLP layer is referred to as __Dense__, which stands for the densely connected layer. Both the first and second MLP layers are identical in nature with 256 units each, followed by __relu activation__ and __dropout__. 256 units are chosen since 128, 512 and 1,024 units have lower performance metrics. At 128 units, the network converges quickly, but has a lower test accuracy. The added number units for 512 or 1,024 does not increase the test accuracy significantly.\n",
        "\n",
        "The number of units is a __hyperparameter__. It controls the __capacity__ of the network. The capacity is a measure of the complexity of the function that the network can approximate. For example, for polynomials, the degree is the hyperparameter. As the degree increases, the capacity of the function also increases. As shown in the following model, the classifier model is implemented using a sequential model API of Keras. This is sufficient if the model requires one input and one output processed by a sequence of layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCyXxlMFqYQJ",
        "colab_type": "text"
      },
      "source": [
        "Since a `Dense` layer is a linear operation, a sequence of `Dense` layers can only approximate a linear function. The problem is that the MNIST digit classification is inherently a non-linear process. Inserting a `relu` activation between `Dense` layers will enable MLPs to model non-linear mappings. `relu` or __Rectified Linear Unit (ReLU)__ is a simple non-linear function. It's very much like a filter that allows positive inputs to pass through unchanged while clamping everything else to zero. Mathematically, `relu` is expressed in the following equation:\n",
        "\n",
        "$$\n",
        "\\text{relu}(x) = \\text{max}(0, x)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfLRUtSSq9Ki",
        "colab_type": "text"
      },
      "source": [
        "There are other non-linear functions that can be used such as `elu`, `selu`, `softplus`, `sigmoid`, and `tanh`. However, relu is the most commonly used in the industry and is computationally efficient due to its simplicity. The `sigmoid` and `tanh` are used as activation functions in the output layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9ba9aCprOT0",
        "colab_type": "text"
      },
      "source": [
        "## Regularization\n",
        "A neural network has the tendency to memorize its training data especially if it contains more than enough capacity. In such a case, the network fails catastrophically when subjected to the test data. To avoid this tendency, the model uses a regularizing layer or function. A common regularizing layer is referred to as a __dropout__.\n",
        "\n",
        "The idea of dropout: given a dropout rate (above, it is set to `dropout=0.45`), the `Dropout` layer randomly removes that fraction of units from participating in the next layer. For example, if the first layer has 256 units, after `dropout=0.45` is applied, only (1 - 0.45) * 256 units = 140 units from layer 1 participate in layer 2. The `Dropout` layer makes neural networks robust to unforeseen input data because the network is trained to predict correctly, even if some units are missing. Dropout is not used in the output layer and it is only active during training. Moreover, dropout is not present during prediction.\n",
        "\n",
        "There are regularizers that can be used other than dropouts like `l1` or `l2`. In Keras, the bias, weight and activation output can be regularized per layer. `l1` and `l2` favor smaller parameter values by adding a penalty function. Both `l1` and `l2` enforce the penalty using a fraction of the sum of absolute (`l1`) or square (`l2`) of parameter values. In other words, the penalty function forces the optimizer to find parameter values that are small. Neural networks with small parameter values are more insensitive to the presence of noise from within the input data.\n",
        "\n",
        "Example of `l2` weight regularizer with `fraction=0.001`:\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "model.add(Dense(hidden_units,\n",
        "                kernel_regularizer = l1(0.001),\n",
        "                input_dim = input_size))\n",
        "```\n",
        "No additional layer is added if `l1` or `l2` regularization is used. The regularization is imposed in the `Dense` layer internally. For the proposed model, dropout still has a better performance than `l2`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5okdrekntl21",
        "colab_type": "text"
      },
      "source": [
        "## Output Activation and Loss Function\n",
        "\n",
        "The output layer has 10 units followed by `softmax` activation. The 10 units correspond to the 10 possible labels, classes or categories. The `softmax` activation can be expressed mathematically as shown in the following equation:\n",
        "$$\n",
        "\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=0}^{N-1}e^{x_j}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gqd2gl4dvLqy",
        "colab_type": "text"
      },
      "source": [
        "There are other choices of output activation layer, like `linear`, `sigmoid`, and `tanh`. The linear activation is an identity function. It copies its input to its output. The `sigmoid` function is more specifically known as a __logistic sigmoid__. This will be used if the elements of the prediction tensor should be mapped between 0.0 and 1.0 independently. The summation of all elements of the predicted tensor is not constrained to 1.0 unlike in `softmax`. For example, `sigmoid` is used as the last layer in sentiment prediction (0.0 is bad to 1.0, which is good) or in image generation (0.0 is 0 to 1.0 is 255-pixel values).\n",
        "\n",
        "The `tanh` function maps its input in the range -1.0 to 1.0. This is important if the output can swing in both positive and negative values. The `tanh` function is more popularly used in the internal layer of recurrent neural networks but has also been used as output layer activation. If `tanh` is used to replace `sigmoid` in the output activation, the data used must be scaled appropriately. For example, instead of scaling each grayscale pixel in the range [0.0  1.0] using:\n",
        "$$\n",
        "x = \\frac{x}{255},\n",
        "$$\n",
        "\n",
        "it is assigned in the range [-1.0, 1.0] by\n",
        "$$\n",
        "x = \\frac{x - 127.5}{127.5}\n",
        "$$\n",
        "\n",
        "The following graph shows the `sigmoid` and `tanh` functions. Mathematically, `sigmoid` can be expressed in equation as follows:\n",
        "$$\n",
        "\\text{sigmoid}(x) = \\sigma(x)=\\frac{1}{1+e^{-x}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80_besfpwjAE",
        "colab_type": "text"
      },
      "source": [
        "How far the predicted tensor is from the one-hot ground truth vector is called __loss__. One type of loss function is `mean_squared_error`__(mse)__, or the average of the squares of the differences between target and prediction. In the current example, we are using `categorical_crossentropy`. It's the negative of the sum of the product of the target and the logarithm of the prediction. There are other loss functions that are available in Keras, such as `mean_absolute_error`, and `binary_crossentropy`. The choice of the loss function is not arbitrary but should be a criterion that the model is learning. For classification by category, `categorical_crossentropy` or `mean_squared_error` is a good choice after the softmax activation layer. The `binary_crossentropy` loss function is normally used after the `sigmoid` activation layer while `mean_squared_error` is an option for `tanh` output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AwJ3QO6xB8E",
        "colab_type": "text"
      },
      "source": [
        "## Optimization\n",
        "\n",
        "With optimization, the objective is to minimize the loss function. The idea is that if the loss is reduced to an acceptable level, the model has indirectly learned the function mapping input to output. Performance metrics are used to determine if a model has learned the underlying data distribution. The default metric in Keras is __loss__. During training, validation, and testing, other metrics such as __accuracy__ can also be included. Accuracy is the percent, or fraction, of correct predictions based on ground truth. In deep learning, there are many other performance metrics. However, it depends on the target application of the model.\n",
        "\n",
        "In Keras, there are several choices for optimizers. The most commonly used optimizers are; __Stochastic Gradient Descent (SGD)__, __Adaptive Moments (Adam)__, and __Root Mean Squared Propagation (RMSprop)__. Each optimizer features tunable parameters like learning rate, momentum, and decay. Adam and RMSprop are variations of SGD with adaptive learning rates. In the proposed classifier network, Adam is used since it has the highest test accuracy.\n",
        "\n",
        "SGD is considered the most fundamental optimizer. It's a simpler version of the gradient descent in calculus. In __gradient descent (GD)__, tracing the curve of a function downhill finds the minimum value, much like walking downhill in a valley or opposite the gradient until the bottom is reached.\n",
        "\n",
        "Let's suppose x is the parameter (for example, weight) being tuned to find the minimum value of y (for example, loss function). Starting at an arbitrary point of x = -0.5 with the gradient being.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqO04-L7q5Ar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}