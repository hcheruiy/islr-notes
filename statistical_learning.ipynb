{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2 - Statistical Learning\n",
    "__Inputs__, also known as _predictors, independent variables, features_, or more generally, _variables_.\n",
    "\n",
    "__Outputs__, also known as response or dependent variable.\n",
    "\n",
    "Suppose an observed quantitative response $\\mathbf{Y}$ and $p$ different predictors $x_1,x_2, \\dots,x_p$. The assumed relationship between $\\mathbf{Y}$ and $\\mathbf{X}=(x_1,x_2,\\ldots,x_p)$ can be generalized as:\n",
    "\n",
    "$$\\mathbf{Y}=f(\\mathbf{X})+\\epsilon$$\n",
    "where $f$ is some fixed, but unknown function of $\\mathbf{X}$ and $\\epsilon$ is a _random error term_ that is independent of $\\mathbf{X}$ and has a mean of zero. In such a scenario, $f$ represents the systematic information that $\\mathbf{X}$ provides about $\\mathbf{Y}$.\n",
    "\n",
    "In general, an estimation of $f$, denoted by $\\hat{f}$, will not be perfect and will introduce error.\n",
    "\n",
    "The error introduced by the discrepancy between $f$ and $\\hat{f}$ is known as __irreducible error__ because it can never be reduced regardless of the accuracy $\\hat{f}$.\n",
    "\n",
    "The irreducible error will be larger than zero because $\\epsilon$ may contain unmeasured variables needed to predict $\\mathbf{Y}$ or ϵ may contain unmeasured variation. The irreducible error always enforces an upper bound on the accuracy of predicting $\\mathbf{Y}$. In practice, this bound is almost always unknown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating $f$\n",
    "\n",
    "### Parametric Methods\n",
    "__Parametric methods__ utilize a two-step model-based approach.\n",
    "\n",
    "1. First, make an assumption about the functional nature, or shape, of $f$. For example, assume that $f$ is linear, yielding a linear model.\n",
    "\n",
    "2. Once a model has been selected, use training data to fit, or train, the model. In the case of a linear model of the form\n",
    "$$f(x)=\\beta_0+\\beta_1x_1+\\beta_2x_2+\\dots+\\beta_px_p$$,\n",
    "the training procedure should yield estimates for the parameters $\\beta_0, \\beta_1, \\dots, \\beta_p$ such that\n",
    "\n",
    "$$\\mathbf{Y} \\approx f(\\mathbf{X}) \\approx \\beta_0 + \\beta_1x_1 + \\beta_2x_2+ \\dots + \\beta_px_p$$.\n",
    "A model-based approach like that outlined above is referred to as __parametric__ because it simplifies the problem of estimating f down to estimating a set of parameters.\n",
    "\n",
    "In general, it is much simpler to estimate a set of parameters than it is to estimate an entirely arbitrary function $f$. A disadvantage of this approach is that the specified model won’t usually match the true form of $f$.\n",
    "\n",
    "Using more flexible models is one means to attempt to combat inaccuracies in the chosen model. However, more flexible models have the disadvantage of requiring a greater number of parameters to be estimated and they are also more susceptible to __overfitting__.\n",
    "\n",
    "_Overfitting_ is a phenomenon where a model closely matches the training data such that it captures too much of the noise or error in the data. This results in a model that fits the training data very well, but doesn’t make good predictions under test or in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Parametric Methods\n",
    "__Non-parametric methods__ don’t make explicit assumptions about $f$ and instead seek to estimate $f$ by getting as close to the data points as possible without being too coarse or granular, preferring smoothness instead.\n",
    "\n",
    "Non-parametric approaches can fit a wider range of possible shapes for f since essentially no assumptions about the form of f are made. However, since non-parametric approaches don’t simplify the problem of estimating $f$, they tend to require a very large number of observations to accurately estimate $f$.\n",
    "\n",
    "A thin-plate spline is one example of a non-parametric method.\n",
    "\n",
    "Though less flexible, more restrictive models are more limited in the shapes they can estimate, they are easier to interpret because the relation of the predictors to the output is more easily understood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning vs. Unsupervised Learning\n",
    "__Supervised learning__ refers to those scenarios in which for each observation of the predictor measurements $\\mathbf{X}_i$ there is an associated response measurement $\\mathbf{Y}_i$. In such a scenario, it is often desirable to generate a model that relates the predictors to the response with the goal of accurately predicting future observations or of better inferring the relationship between the predictors and the response.\n",
    "\n",
    "__Unsupervised learning__ refers to those scenarios in which for each observation of the predictor measurements $\\mathbf{X}_i$, there is no associated response $\\mathbf{Y}_i$. This is referred to as unsupervised because there is no response variable that can supervise the analysis that goes into generating a model.\n",
    "\n",
    "__Cluster analysis__, a process by which observations are arranged into relatively distinct groups, is one form of unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Problems vs. Classification Problems\n",
    "__Quantitative values__, whether a variable or response, take on numerical values. Problems with a quantitative response are often referred to as __regression problems__.\n",
    "\n",
    "__Qualitative values__, whether a variable or response, take on values in one of $\\mathbf{K}$ different classes or categories. Problems with a qualitative response are often referred to as classification problems.\n",
    "\n",
    "Which statistical learning method is best suited to a problem tends to depend on whether the response is qualitative or quantitative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Quality of Fit\n",
    "To evaluate the performance of a model it is necessary to evaluate how well the model’s predictions match the observed data, to quantify to what extent the predicted response is close to the observed response data.\n",
    "\n",
    "__Mean squared error__ is one common measure in the regression setting.\n",
    "\n",
    "Mean squared error is defined as\n",
    "\n",
    "$$\\frac{1}{n}\\sum_{i=1}^n\\bigg(y_i - \\hat{f}(x_i)\\bigg)^2$$\n",
    "\n",
    "The mean squared error will be small when the predicted responses are close to the true responses and large if there’s a substantial difference between the predicted response and the observed response for some observations.\n",
    "\n",
    "Mean squared error is applied both when training a model and when testing a model.\n",
    "\n",
    "Though it may be tempting to optimize the __training mean squared error__, the reality is that the model is judged by the accuracy of its predictions against unseen test data. As such, the model that yields the best _test mean squared error_ is preferable to the model that yields the best training mean squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bias-Variance Trade-Off\n",
    "The number of __degrees of freedom__ quantifies the number of values in the model that are free to vary. The degrees of freedom is a quality that summarizes the flexibility of a curve.\n",
    "\n",
    "As a model’s flexibility increases, the training mean squared error will decrease, but the test mean squared error may not. When the training mean squared error is small but the test mean squared error is large, the model is described as overfitting the data. That said, the training mean squared error will almost always be less than the test mean squared error because most methods either directly or indirectly aim to minimize the training mean squared error.\n",
    "\n",
    "Overfitting refers specifically to scenarios in which a less flexible model would have yielded a smaller test mean squared error.\n",
    "\n",
    "The expected test mean squared error for a given value $x_0$ can be decomposed into the sum of three quantities: The variance of $\\hat{f}(x_0)$, the squared bias of $\\hat{f}(x_0)$, and the variance of the error term, $\\epsilon$. Formally,\n",
    "\n",
    "$$\\mathbf{E}(y_i - \\hat{f}(x_i))^2 = \\text{Var}(\\hat{f}(x_i)) + [\\text{Bias}(\\hat{f}(x_i))]^2 + \\text{Var}(\\epsilon)$$\n",
    "\n",
    "To minimize the expected test error, it’s necessary to choose a method that achieves both low variance and low bias. It can be seen that the expected test mean squared error can never be less than $\\text{Var}(\\epsilon)$, the irreducible error.\n",
    "\n",
    "__Variance__ refers to the amount by which $\\hat{f}$ would change if it were estimated using a different training data set. In general, more flexible methods have higher variance.\n",
    "\n",
    "Bias refers to the error that is introduced by approximating a potentially complex function using a simple model. More flexible models tend to have less bias.\n",
    "\n",
    "In general, the more flexible the statistical learning method, the more variance will increase and bias decrease.\n",
    "\n",
    "The relationship between bias, variance, and test set mean squared error is referred to as the __bias-variance trade-off__. It is called a trade-off because it is a challenge to find a model that has both a low variance and a low squared bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing Classification Accuracy\n",
    "In classification scenarios, the most common means of quantifying the accuracy of $\\hat{f}$ is the _training error rate_. The training error rate is the proportion of errors that are made when applying $\\hat{f}$ to the training observations. Formally stated as,\n",
    "\n",
    "$$\\frac{1}{n}\\sum_{i=1}^n\\iota(y_i \\neq \\hat{y}$$\n",
    "\n",
    "where $\\iota$ is an indicator variable that equals 0 when $y=\\hat{y}$ and equals 1 when $y_i \\neq \\hat{y}$.\n",
    "In simple terms, the error rate is the ratio of incorrect classifications to the observation count.\n",
    "\n",
    "As in the regression scenario, a good classifier is one for which the test error rate is smallest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bayes Classifier\n",
    "It is possible to show that the test error rate is minimized on average by a very simple classifier that assigns each observation to the most likely class given its predictor variables.\n",
    "\n",
    "In Bayesian terms, a test observation should be classified for the predictor vector $x_0$ to the class $j$ for which\n",
    "\n",
    "$$\\text{Pr}(\\mathbf{Y}=j|\\mathbf{X}=x_0)$$\n",
    "is largest. That is, the class for which the conditional probability that $\\mathbf{Y}=j$, given the observed predictor vector $x_0$, is largest. This classifier is called the __Bayes classifier__.\n",
    "\n",
    "In a two-class scenario, this can be restated as $\\text{Pr}(\\mathbf{Y=1}|\\mathbf{X}=x_0)> 0.5$ matching class A when true and class B when false.\n",
    "\n",
    "The threshold where the classification probability is exactly 50% is known as the _Bayes decision boundary_.\n",
    "\n",
    "The Bayes classifier yields the lowest possible test error rate since it will always choose the class with the highest probability. The _Bayes error rate_ can be stated formally as\n",
    "\n",
    "$$1 - \\mathbf{E}(\\text{max}_j\\text{Pr}(\\mathbf{Y} = j|\\mathbf{X}))$$\n",
    "The Bayes error rate can also be described as the ratio of observations that lie on the “wrong” side of the decision boundary.\n",
    "\n",
    "Unfortunately, the conditional distribution of $\\mathbf{Y}$ given $\\mathbf{X}$ is often unknown, so the Bayes classifier is most often unattainable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors\n",
    "Many modeling techniques try to compute the conditional distribution of $\\mathbf{Y}$ given $\\mathbf{X}$ and then provide estimated classifications based on the highest estimated probability. The __K-nearest neighbors classifier__ is one such method.\n",
    "\n",
    "The K-nearest neighbors classifier takes a positive integer $mathbf{K}$ and first identifies the $K$ points that are nearest to $x_0$, represented by $N_0$. It next estimates the conditional probability for class $j$ based on the fraction of points in N0 who have a response equal to j. Formally, the estimated conditional probability can be stated as\n",
    "\n",
    "$$Pr(\\mathbf{Y}=j|\\mathbf{X}=x_0)=\\frac{1}{k}\\sum_{i \\in N_0}\\iota(y_i = j)$$\n",
    "The K-Nearest Neighbor classifier then applies Bayes theorem and yields the classification with the highest probability.\n",
    "\n",
    "Despite its simplicity, the K-Nearest Neighbor classifier often yields results that are surprisingly close to the optimal Bayes classifier.\n",
    "\n",
    "The choice of K can have a drastic effect on the yielded classifier. Too low a K yields a classifier that is too flexible, has too high a variance, and low bias.\n",
    "\n",
    "Conversely, as K increases, the yielded classifier becomes less flexible, with a low variance, but high bias.\n",
    "\n",
    "In both regression and classification scenarios, choosing the correct level of flexibility is critical to the success of the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
